"""
MediaCloud Search Cache - Caches MediaCloud API search results
"""

import hashlib
import json
import time
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta, date


class DateAwareJSONEncoder(json.JSONEncoder):
    """JSON encoder that handles date objects"""
    def default(self, obj):
        if isinstance(obj, date):
            return obj.isoformat()
        elif isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)


class MediaCloudSearchCache:
    """Caches MediaCloud API search results to avoid repeated API calls"""
    
    def __init__(self, cache_root: Path):
        self.cache_root = cache_root / "mediacloud_searches"
        self.cache_root.mkdir(parents=True, exist_ok=True)
        
        # Cache metadata file
        self.metadata_file = self.cache_root / "cache_metadata.json"
        self.metadata = self._load_metadata()
    
    def _load_metadata(self) -> Dict[str, Any]:
        """Load cache metadata"""
        if self.metadata_file.exists():
            try:
                return json.loads(self.metadata_file.read_text())
            except (json.JSONDecodeError, FileNotFoundError):
                return {"searches": {}, "last_cleanup": None}
        return {"searches": {}, "last_cleanup": None}
    
    def _save_metadata(self):
        """Save cache metadata"""
        self.metadata_file.write_text(json.dumps(self.metadata, indent=2, cls=DateAwareJSONEncoder))
    
    def _generate_cache_key(self, query: str, collection_id: int, start_date, end_date) -> str:
        """Generate a unique cache key for a search"""
        # Convert dates to strings if they're date objects
        start_date_str = str(start_date) if hasattr(start_date, 'isoformat') else str(start_date)
        end_date_str = str(end_date) if hasattr(end_date, 'isoformat') else str(end_date)
        
        # Create a deterministic key based on search parameters
        key_data = {
            "query": query,
            "collection_id": collection_id,
            "start_date": start_date_str,
            "end_date": end_date_str
        }
        key_string = json.dumps(key_data, sort_keys=True, cls=DateAwareJSONEncoder)
        return hashlib.sha256(key_string.encode()).hexdigest()[:16]
    
    def _get_cache_file_path(self, cache_key: str) -> Path:
        """Get the file path for a cache entry"""
        return self.cache_root / f"search_{cache_key}.json"
    
    def get_search_results(self, query: str, collection_id: int, start_date, end_date, 
                          max_age_hours: int = 24) -> Optional[List[Dict[str, Any]]]:
        """
        Get cached search results if they exist and are not too old
        
        Args:
            query: Search query
            collection_id: MediaCloud collection ID
            start_date: Start date for search
            end_date: End date for search
            max_age_hours: Maximum age of cached results in hours
            
        Returns:
            Cached search results or None if not found/too old
        """
        cache_key = self._generate_cache_key(query, collection_id, start_date, end_date)
        cache_file = self._get_cache_file_path(cache_key)
        
        if not cache_file.exists():
            return None
        
        # Check if cache entry exists in metadata
        if cache_key not in self.metadata["searches"]:
            return None
        
        cache_entry = self.metadata["searches"][cache_key]
        cache_time = datetime.fromisoformat(cache_entry["cached_at"])
        
        # Check if cache is too old
        if datetime.now() - cache_time > timedelta(hours=max_age_hours):
            print(f"üóëÔ∏è  Cache expired for query: {query[:50]}...")
            self._remove_cache_entry(cache_key)
            return None
        
        # Load and return cached results
        try:
            cached_data = json.loads(cache_file.read_text())
            print(f"üìã Using cached results for query: {query[:50]}... ({len(cached_data['stories'])} stories)")
            return cached_data["stories"]
        except (json.JSONDecodeError, KeyError) as e:
            print(f"‚ö†Ô∏è  Error reading cache file: {e}")
            self._remove_cache_entry(cache_key)
            return None
    
    def cache_search_results(self, query: str, collection_id: int, start_date, end_date, 
                           stories: List[Dict[str, Any]]):
        """
        Cache search results
        
        Args:
            query: Search query
            collection_id: MediaCloud collection ID
            start_date: Start date for search
            end_date: End date for search
            stories: List of story dictionaries to cache
        """
        cache_key = self._generate_cache_key(query, collection_id, start_date, end_date)
        cache_file = self._get_cache_file_path(cache_key)
        
        # Convert dates to strings if they're date objects
        start_date_str = str(start_date) if hasattr(start_date, 'isoformat') else str(start_date)
        end_date_str = str(end_date) if hasattr(end_date, 'isoformat') else str(end_date)
        
        # Prepare cache data
        cache_data = {
            "query": query,
            "collection_id": collection_id,
            "start_date": start_date_str,
            "end_date": end_date_str,
            "stories": stories,
            "cached_at": datetime.now().isoformat(),
            "story_count": len(stories)
        }
        
        # Save to file
        cache_file.write_text(json.dumps(cache_data, indent=2, cls=DateAwareJSONEncoder))
        
        # Update metadata
        self.metadata["searches"][cache_key] = {
            "query": query,
            "collection_id": collection_id,
            "start_date": start_date_str,
            "end_date": end_date_str,
            "story_count": len(stories),
            "cached_at": datetime.now().isoformat(),
            "cache_file": str(cache_file)
        }
        self._save_metadata()
        
        print(f"üíæ Cached {len(stories)} stories for query: {query[:50]}...")
    
    def _remove_cache_entry(self, cache_key: str):
        """Remove a cache entry and its file"""
        if cache_key in self.metadata["searches"]:
            cache_file = self._get_cache_file_path(cache_key)
            if cache_file.exists():
                cache_file.unlink()
            del self.metadata["searches"][cache_key]
            self._save_metadata()
    
    def clear_cache(self, older_than_hours: Optional[int] = None):
        """
        Clear cache entries
        
        Args:
            older_than_hours: If provided, only clear entries older than this many hours
        """
        if older_than_hours is None:
            # Clear all entries
            for cache_key in list(self.metadata["searches"].keys()):
                self._remove_cache_entry(cache_key)
            print(f"üóëÔ∏è  Cleared all MediaCloud search cache entries")
        else:
            # Clear only old entries
            cutoff_time = datetime.now() - timedelta(hours=older_than_hours)
            old_entries = []
            
            for cache_key, entry in self.metadata["searches"].items():
                cache_time = datetime.fromisoformat(entry["cached_at"])
                if cache_time < cutoff_time:
                    old_entries.append(cache_key)
            
            for cache_key in old_entries:
                self._remove_cache_entry(cache_key)
            
            print(f"üóëÔ∏è  Cleared {len(old_entries)} MediaCloud search cache entries older than {older_than_hours} hours")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        total_entries = len(self.metadata["searches"])
        total_stories = sum(entry["story_count"] for entry in self.metadata["searches"].values())
        
        # Calculate cache size
        cache_size = 0
        for cache_file in self.cache_root.glob("search_*.json"):
            if cache_file.is_file():
                cache_size += cache_file.stat().st_size
        
        return {
            "total_entries": total_entries,
            "total_stories": total_stories,
            "cache_size_bytes": cache_size,
            "cache_size_mb": round(cache_size / (1024 * 1024), 2),
            "last_cleanup": self.metadata.get("last_cleanup")
        }
    
    def list_cached_searches(self) -> List[Dict[str, Any]]:
        """List all cached searches"""
        searches = []
        for cache_key, entry in self.metadata["searches"].items():
            searches.append({
                "cache_key": cache_key,
                "query": entry["query"],
                "collection_id": entry["collection_id"],
                "date_range": f"{entry['start_date']} to {entry['end_date']}",
                "story_count": entry["story_count"],
                "cached_at": entry["cached_at"]
            })
        return searches
